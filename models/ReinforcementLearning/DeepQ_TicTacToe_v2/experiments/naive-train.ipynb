{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent\n",
    "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n",
    "from models.ReinforcementLearning.Utils import (\n",
    "    train_agent,\n",
    "    test_agent,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "SEED = 100\n",
    "set_seed(SEED)\n",
    "\n",
    "# DeepQ parameters\n",
    "BATCH_SIZE     = 128\n",
    "NUM_EPISODES   = 2000 if torch.cuda.is_available() else 100\n",
    "STATE_SPACE    = 9\n",
    "ACTION_SPACE   = 9\n",
    "HIDDEN_SIZE    = 128\n",
    "EPSILON        = 1.0\n",
    "GAMMA          = 0.99\n",
    "LEARNING_RATE  = 0.001\n",
    "DROPOUT        = 0.25\n",
    "TRAIN_START    = 1500\n",
    "NEGATIVE_SLOPE = 0.01\n",
    "COMPUTER_LEVEL = OPPONENT_LEVEL.NAIVE\n",
    "\n",
    "# save path\n",
    "MODEL_PATH = \"../../../../trained_models/ReinforcementLearning/TicTacToeV2\"\n",
    "MODEL_NAME = \"\"\n",
    "\n",
    "BASELINE = \"TicTacToev2-Baseline-Untrained\"\n",
    "NAIVE_2K = \"TicTacToev2-NAIVE-2K\"\n",
    "NAIVE_4K = \"TicTacToev2-NAIVE-4K\"\n",
    "NAIVE_6K = \"TicTacToev2-NAIVE-6K\"\n",
    "NAIVE_8K = \"TicTacToev2-NAIVE-8K\"\n",
    "NAIVE_10K = \"TicTacToev2-NAIVE-10K\"\n",
    "\n",
    "def get_full_model_path(agent_name: str = None):\n",
    "  if agent_name is None:\n",
    "    return os.path.join(MODEL_PATH, MODEL_NAME + \".pt\")\n",
    "  return os.path.join(MODEL_PATH, agent_name + \".pt\")\n",
    "\n",
    "def supply_model(load_if_exists: bool = True, agent_name: str = None):\n",
    "  \n",
    "  agent = DeepQAgent(\n",
    "      device         = DEVICE,\n",
    "      epsilon        = EPSILON, \n",
    "      gamma          = GAMMA,\n",
    "      state_space    = STATE_SPACE, \n",
    "      action_space   = ACTION_SPACE, \n",
    "      hidden_size    = HIDDEN_SIZE,\n",
    "      dropout        = DROPOUT,\n",
    "      train_start    = TRAIN_START,\n",
    "      batch_size     = BATCH_SIZE,\n",
    "      negative_slope = NEGATIVE_SLOPE\n",
    "  )\n",
    "\n",
    "  full_model_path = get_full_model_path(agent_name)\n",
    "  if load_if_exists and os.path.exists(full_model_path):\n",
    "    print(\"Loading Model Parameters...\")\n",
    "    agent.load_model(filepath=full_model_path)\n",
    "  \n",
    "  optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "  criterion = torch.nn.SmoothL1Loss() # Huber Loss\n",
    "  \n",
    "  return agent, optimizer, criterion\n",
    "\n",
    "def compare_to_naive(agent_name: str, num_episodes: int = 25000):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n",
    "\n",
    "def compare_to_optimal(agent_name: str, num_episodes: int = 100):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=BASELINE)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE_2K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=NAIVE_2K)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE_4K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=NAIVE_4K)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE_6K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=NAIVE_6K)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE_8K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=NAIVE_8K)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE_10K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the NAIVE models to a Naive opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(NAIVE_2K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(NAIVE_4K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(NAIVE_6K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(NAIVE_8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_naive(NAIVE_10K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing NAIVE models to Optimal opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(NAIVE_2K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(NAIVE_4K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(NAIVE_6K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(NAIVE_8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to_optimal(NAIVE_10K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NAIVE_2K and NAIVE_4K models seem to perform well against the the Naive opponent, and better than the others against the Optimal Opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
