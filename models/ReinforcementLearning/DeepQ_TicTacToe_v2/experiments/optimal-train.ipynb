{"cells":[{"cell_type":"markdown","metadata":{"id":"XReCdM8kILUt"},"source":["#### Optimal Opponent Experiments\n","Author: Yemi Kelani"]},{"cell_type":"markdown","metadata":{"id":"1vNSWFUkwdXc"},"source":["##### Google Drive Setup (Skip if running locally)\n","\n","> To run this notebook, follow these steps:\n","> 1. Download the latest version of the [repository](https://github.com/yemi-kelani/artificial-intelligence/tree/master).\n","> 2. Upload the repsitory files to your Google Drive account under the path `Projects/artificial-intelligence`.\n","> 3. Open this file (`optimal-train.ipynb`) from your Google Drive and run the experiments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEqTNnEdl-8u"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":369,"status":"ok","timestamp":1730671211694,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"3IlDuGfgof5R"},"outputs":[],"source":["ROOT_FOLDER = \"drive/MyDrive/Projects/artificial-intelligence/models/ReinforcementLearning/\"\n","PROJECT_PATH = f\"{ROOT_FOLDER}/DeepQ_TicTacToe_v2\"\n","NOTEBOOK_LOCATION = f\"{PROJECT_PATH}/experiments\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":368,"status":"ok","timestamp":1730671249730,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"5dji1umepw8Z"},"outputs":[],"source":["!cp {PROJECT_PATH}/DeepQAgent.py .\n","!cp {PROJECT_PATH}/TicTacToeGame.py .\n","!cp {ROOT_FOLDER}/Utils.py .\n","\n","from DeepQAgent import DeepQAgent\n","from  TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n","from Utils import (\n","    train_agent,\n","    test_agent,\n","    set_seed\n",")\n","MODEL_PATH = \"drive/MyDrive/Projects/artificial-intelligence/trained_models/ReinforcementLearning/TicTacToeV2\""]},{"cell_type":"markdown","metadata":{"id":"rOHKJqHY22Ga"},"source":["##### Local Setup (Skip if running remotely)\n","> 1. Run the following cells"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8BLUQ7N3Ors"},"outputs":[],"source":["from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent\n","from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n","from models.ReinforcementLearning.Utils import (\n","    train_agent,\n","    test_agent,\n","    set_seed\n",")\n","MODEL_PATH = \"../../../../trained_models/ReinforcementLearning/TicTacToeV2\""]},{"cell_type":"markdown","metadata":{"id":"2kw4XZB63VyB"},"source":["##### Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1730679656097,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"IpvCCHUX3Ge2","outputId":"a461af1f-fa40-4cef-eb31-dc4d954d454c"},"outputs":[],"source":["import os\n","import torch\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)\n","\n","SEED = 100\n","set_seed(SEED)\n","\n","# DeepQ parameters\n","BATCH_SIZE     = 128\n","NUM_EPISODES   = 1000 if torch.cuda.is_available() else 100\n","STATE_SPACE    = 9\n","ACTION_SPACE   = 9\n","HIDDEN_SIZE    = 128\n","EPSILON        = 1.0\n","GAMMA          = 0.99\n","LEARNING_RATE  = 0.001\n","DROPOUT        = 0.25\n","TRAIN_START = 1000\n","NEGATIVE_SLOPE = 0.01\n","COMPUTER_LEVEL = OPPONENT_LEVEL.NAIVE\n","\n","# save path\n","MODEL_NAME = \"\"\n","\n","BASELINE = \"TicTacToev2-Baseline-Untrained\"\n","NAIVE_2K = \"TicTacToev2-NAIVE-2K\"\n","NAIVE_4K = \"TicTacToev2-NAIVE-4K\"\n","NAIVE_6K = \"TicTacToev2-NAIVE-6K\"\n","NAIVE_8K = \"TicTacToev2-NAIVE-8K\"\n","NAIVE_10K = \"TicTacToev2-NAIVE-10K\"\n","AGENT_1K = \"TicTacToev2-AGENT-1K\"\n","AGENT_2K = \"TicTacToev2-AGENT-2K\"\n","AGENT_3K = \"TicTacToev2-AGENT-3K\"\n","AGENT_4K = \"TicTacToev2-AGENT-4K\"\n","OPTIMAL_1K = \"TicTacToev2-OPTIMAL-1K\"\n","OPTIMAL_2K = \"TicTacToev2-OPTIMAL-2K\"\n","OPTIMAL_4K = \"TicTacToev2-OPTIMAL-4K\"\n","OPTIMAL_6K = \"TicTacToev2-OPTIMAL-6K\"\n","OPTIMAL_8K = \"TicTacToev2-OPTIMAL-8K\"\n","\n","\n","def get_full_model_path(agent_name: str = None):\n","  if agent_name is None:\n","    return os.path.join(MODEL_PATH, MODEL_NAME + \".pt\")\n","  return os.path.join(MODEL_PATH, agent_name + \".pt\")\n","\n","def supply_model(load_if_exists: bool = True, agent_name: str = None):\n","\n","  agent = DeepQAgent(\n","      device         = DEVICE,\n","      epsilon        = EPSILON,\n","      gamma          = GAMMA,\n","      state_space    = STATE_SPACE,\n","      action_space   = ACTION_SPACE,\n","      hidden_size    = HIDDEN_SIZE,\n","      dropout        = DROPOUT,\n","      train_start    = TRAIN_START,\n","      batch_size     = BATCH_SIZE,\n","      negative_slope = NEGATIVE_SLOPE\n","  )\n","\n","  full_model_path = get_full_model_path(agent_name)\n","  if load_if_exists and os.path.exists(full_model_path):\n","    print(\"Loading Model Parameters...\")\n","    agent.load_model(filepath=full_model_path)\n","\n","  optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n","  criterion = torch.nn.SmoothL1Loss() # Huber Loss\n","\n","  return agent, optimizer, criterion\n","\n","def compare_to_naive(agent_name: str, num_episodes: int = 25000):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n","\n","def compare_to_optimal(agent_name: str, num_episodes: int = 100):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n","\n","def compare_to_model(agent_name: str, model_name: str, num_episodes: int = 10000):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _enemy, _, _ = supply_model(load_if_exists=True, agent_name=model_name)\n","  _environment = TicTacToeGame(DEVICE, _enemy, OPPONENT_LEVEL.AGENT, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4687666,"status":"ok","timestamp":1730529008684,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"a8fX711p3rw_","outputId":"a2acf835-d7ac-4366-b7e2-a8efa3058c7c"},"outputs":[],"source":["agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=AGENT_1K)\n","environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n","reward_history = train_agent(\n","    agent,\n","    environment,\n","    NUM_EPISODES,\n","    optimizer,\n","    criterion,\n","    DEVICE,\n","    MODEL_PATH,\n","    model_name = OPTIMAL_1K\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4712505,"status":"ok","timestamp":1730534554286,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"wPejXGexnUeU","outputId":"58db5e25-de6d-40b1-c164-f2e619ebb7e5"},"outputs":[],"source":["agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=OPTIMAL_1K)\n","environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n","reward_history = train_agent(\n","    agent,\n","    environment,\n","    NUM_EPISODES,\n","    optimizer,\n","    criterion,\n","    DEVICE,\n","    MODEL_PATH,\n","    model_name = OPTIMAL_2K\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194739,"status":"ok","timestamp":1730536535721,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"wOhqfsrYAI1B","outputId":"065004a2-075e-443f-cb48-5062db7aac5c"},"outputs":[],"source":["compare_to_naive(OPTIMAL_1K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169322,"status":"ok","timestamp":1730536705040,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"rQ6P1wzxAP0a","outputId":"79bed629-bfbb-4445-b186-dd6ba5b6046d"},"outputs":[],"source":["compare_to_naive(OPTIMAL_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383600,"status":"ok","timestamp":1730537088635,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"DHqM7OPVAR0T","outputId":"b0f8caf4-374b-4e6a-dbd1-25bb29d9daa7"},"outputs":[],"source":["compare_to_optimal(OPTIMAL_1K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379932,"status":"ok","timestamp":1730537468563,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"mbOviJqhAYIC","outputId":"639cb76e-2727-4a37-fd72-7a283fa0e4fc"},"outputs":[],"source":["compare_to_optimal(OPTIMAL_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88772,"status":"ok","timestamp":1730537594071,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"7Pzbpxz7Ae1w","outputId":"7be2a52c-7995-424e-cfc8-6f536c997444"},"outputs":[],"source":["compare_to_model(OPTIMAL_1K, NAIVE_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76124,"status":"ok","timestamp":1730537670190,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"Vu-UUpygAo6L","outputId":"0421c417-64e3-464d-bbc4-b5bf7176c167"},"outputs":[],"source":["compare_to_model(OPTIMAL_2K, NAIVE_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":408,"status":"ok","timestamp":1730671261156,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"65vmgG19ArH-"},"outputs":[],"source":["TRAIN_START = 1000\n","NUM_EPISODES = 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10878554,"status":"ok","timestamp":1730577990342,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":420},"id":"uVn7xDLo1F2Y","outputId":"f679a5b1-0380-437e-bde5-59564bc2c995"},"outputs":[],"source":["agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=OPTIMAL_2K)\n","environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL)\n","reward_history = train_agent(\n","    agent,\n","    environment,\n","    NUM_EPISODES,\n","    optimizer,\n","    criterion,\n","    DEVICE,\n","    MODEL_PATH,\n","    model_name = OPTIMAL_4K\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163407,"status":"ok","timestamp":1730670002465,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"7H79C0Ut9Qr3","outputId":"abad1845-b705-4c34-ac32-611300be88d4"},"outputs":[],"source":["compare_to_naive(OPTIMAL_4K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391761,"status":"ok","timestamp":1730670394224,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"yxX-O9LO9SRn","outputId":"9880d9bf-064d-43b7-cecc-51934f1bfafb"},"outputs":[],"source":["compare_to_optimal(OPTIMAL_4K)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70672,"status":"ok","timestamp":1730670464892,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"NlK9pkR19XV0","outputId":"4fc807ce-e20d-46ae-e78b-65d2321e8807"},"outputs":[],"source":["compare_to_model(OPTIMAL_4K, NAIVE_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77115,"status":"ok","timestamp":1730670542005,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"uNEzk3yX_C-5","outputId":"3cd4be7e-7765-4eb4-c43e-d1e44b47cccc"},"outputs":[],"source":["compare_to_model(OPTIMAL_4K, OPTIMAL_2K)"]},{"cell_type":"markdown","metadata":{"id":"zWKC4izRAvnG"},"source":["The evaluation results for OPTIMAL_4K look promising. That being said, the state outputs from the training loop seem to fixate on attaining certain states (i.e. all Xs on the bottom row)."]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPBd3f2FDh4QBgIji6dTpDa","collapsed_sections":["rOHKJqHY22Ga"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
