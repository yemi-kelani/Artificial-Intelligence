{"cells":[{"cell_type":"markdown","metadata":{"id":"XReCdM8kILUt"},"source":["#### Optimal Opponent Experiments\n","Author: Yemi Kelani"]},{"cell_type":"markdown","metadata":{"id":"1vNSWFUkwdXc"},"source":["##### Google Drive Setup (Skip if running locally)\n","\n","> To run this notebook, follow these steps:\n","> 1. Download the latest version of the [repository](https://github.com/yemi-kelani/artificial-intelligence/tree/master).\n","> 2. Upload the repsitory files to your Google Drive account under the path `Projects/artificial-intelligence`.\n","> 3. Open this file (`self-train.ipynb`) from your Google Drive and run the experiments."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24457,"status":"ok","timestamp":1730739972138,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"rEqTNnEdl-8u","outputId":"1cf14225-78c5-4fb2-d410-97c45f137bbe"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":254,"status":"ok","timestamp":1730739974650,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"3IlDuGfgof5R"},"outputs":[],"source":["ROOT_FOLDER = \"drive/MyDrive/Projects/artificial-intelligence/models/ReinforcementLearning/\"\n","PROJECT_PATH = f\"{ROOT_FOLDER}/DeepQ_TicTacToe_v2\"\n","NOTEBOOK_LOCATION = f\"{PROJECT_PATH}/experiments\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6622,"status":"ok","timestamp":1730739983162,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"5dji1umepw8Z"},"outputs":[],"source":["!cp {PROJECT_PATH}/DeepQAgent.py .\n","!cp {PROJECT_PATH}/TicTacToeGame.py .\n","!cp {ROOT_FOLDER}/Utils.py .\n","\n","from DeepQAgent import DeepQAgent\n","from  TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n","from Utils import (\n","    train_agent,\n","    test_agent,\n","    set_seed\n",")\n","MODEL_PATH = \"drive/MyDrive/Projects/artificial-intelligence/trained_models/ReinforcementLearning/TicTacToeV2\""]},{"cell_type":"markdown","metadata":{"id":"rOHKJqHY22Ga"},"source":["##### Local Setup (Skip if running remotely)\n","> 1. Run the following cells"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8BLUQ7N3Ors"},"outputs":[],"source":["from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent\n","from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n","from models.ReinforcementLearning.Utils import (\n","    train_agent,\n","    test_agent,\n","    set_seed\n",")\n","MODEL_PATH = \"../../../../trained_models/ReinforcementLearning/TicTacToeV2\""]},{"cell_type":"markdown","metadata":{"id":"2kw4XZB63VyB"},"source":["##### Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1730739992917,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"IpvCCHUX3Ge2","outputId":"fed9afb4-2f7e-459c-f8d8-3af8ddcc30c1"},"outputs":[],"source":["import os\n","import torch\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)\n","\n","SEED = 100\n","set_seed(SEED)\n","\n","# DeepQ parameters\n","BATCH_SIZE     = 128\n","NUM_EPISODES   = 10000 if torch.cuda.is_available() else 100\n","STATE_SPACE    = 9\n","ACTION_SPACE   = 9\n","HIDDEN_SIZE    = 128\n","EPSILON        = 1.0\n","GAMMA          = 0.99\n","LEARNING_RATE  = 0.001\n","DROPOUT        = 0.25\n","TRAIN_START = 1500\n","NEGATIVE_SLOPE = 0.01\n","COMPUTER_LEVEL = OPPONENT_LEVEL.NAIVE\n","\n","# save path\n","MODEL_NAME = \"\"\n","\n","BASELINE = \"TicTacToev2-Baseline-Untrained\"\n","NAIVE_2K = \"TicTacToev2-NAIVE-2K\"\n","NAIVE_4K = \"TicTacToev2-NAIVE-4K\"\n","NAIVE_6K = \"TicTacToev2-NAIVE-6K\"\n","NAIVE_8K = \"TicTacToev2-NAIVE-8K\"\n","NAIVE_10K = \"TicTacToev2-NAIVE-10K\"\n","AGENT_1K = \"TicTacToev2-AGENT-1K\"\n","AGENT_2K = \"TicTacToev2-AGENT-2K\"\n","AGENT_3K = \"TicTacToev2-AGENT-3K\"\n","AGENT_4K = \"TicTacToev2-AGENT-4K\"\n","OPTIMAL_1K = \"TicTacToev2-OPTIMAL-1K\"\n","OPTIMAL_2K = \"TicTacToev2-OPTIMAL-2K\"\n","OPTIMAL_4K = \"TicTacToev2-OPTIMAL-4K\"\n","OPTIMAL_6K = \"TicTacToev2-OPTIMAL-6K\"\n","OPTIMAL_8K = \"TicTacToev2-OPTIMAL-8K\"\n","SELF_10K = \"TicTacToev2-SELF-10K\"\n","\n","\n","def get_full_model_path(agent_name: str = None):\n","  if agent_name is None:\n","    return os.path.join(MODEL_PATH, MODEL_NAME + \".pt\")\n","  return os.path.join(MODEL_PATH, agent_name + \".pt\")\n","\n","def supply_model(load_if_exists: bool = True, agent_name: str = None):\n","\n","  agent = DeepQAgent(\n","      device         = DEVICE,\n","      epsilon        = EPSILON,\n","      gamma          = GAMMA,\n","      state_space    = STATE_SPACE,\n","      action_space   = ACTION_SPACE,\n","      hidden_size    = HIDDEN_SIZE,\n","      dropout        = DROPOUT,\n","      train_start    = TRAIN_START,\n","      batch_size     = BATCH_SIZE,\n","      negative_slope = NEGATIVE_SLOPE\n","  )\n","\n","  full_model_path = get_full_model_path(agent_name)\n","  if load_if_exists and os.path.exists(full_model_path):\n","    print(\"Loading Model Parameters...\")\n","    agent.load_model(filepath=full_model_path)\n","\n","  optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n","  criterion = torch.nn.SmoothL1Loss() # Huber Loss\n","\n","  return agent, optimizer, criterion\n","\n","def compare_to_naive(agent_name: str, num_episodes: int = 25000):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n","\n","def compare_to_optimal(agent_name: str, num_episodes: int = 100):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n","\n","def compare_to_model(agent_name: str, model_name: str, num_episodes: int = 10000):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _enemy, _, _ = supply_model(load_if_exists=True, agent_name=model_name)\n","  _environment = TicTacToeGame(DEVICE, _enemy, OPPONENT_LEVEL.AGENT, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17160908,"status":"ok","timestamp":1730707631889,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"a8fX711p3rw_","outputId":"76d719a3-1578-4202-9353-b0d39d78d2b8"},"outputs":[],"source":["agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=OPTIMAL_4K)\n","environment = TicTacToeGame(DEVICE, agent, OPPONENT_LEVEL.AGENT)\n","reward_history = train_agent(\n","    agent,\n","    environment,\n","    NUM_EPISODES,\n","    optimizer,\n","    criterion,\n","    DEVICE,\n","    MODEL_PATH,\n","    model_name = SELF_10K\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182320,"status":"ok","timestamp":1730733957551,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"zgR-Xk_p0BEG","outputId":"2cab8d9b-212a-4eef-9b77-4df58073a7c2"},"outputs":[],"source":["compare_to_naive(SELF_10K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70460,"status":"ok","timestamp":1730735040436,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"0K07GneS0G0X","outputId":"082661be-e9c3-44f4-8c4e-743dcf4e2e0b"},"outputs":[],"source":["compare_to_model(SELF_10K, NAIVE_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381853,"status":"ok","timestamp":1730740383024,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"QlFhmjfeBIob","outputId":"6805a11d-7902-482f-adad-b896f6fdf23e"},"outputs":[],"source":["compare_to_optimal(SELF_10K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93096,"status":"ok","timestamp":1730735133528,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"l5PlP_-y0Izc","outputId":"4146de26-82f4-47dd-9ec2-77532bda4138"},"outputs":[],"source":["compare_to_model(SELF_10K, OPTIMAL_2K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77390,"status":"ok","timestamp":1730735210915,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"az84bU041DrP","outputId":"8ea2c806-d728-42cf-e62b-5ae57e0123ad"},"outputs":[],"source":["compare_to_model(SELF_10K, OPTIMAL_4K)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88945,"status":"ok","timestamp":1730738026333,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"z9tGnNocBJe1","outputId":"fc0104e0-aa2d-401d-c081-c0de2f2195ff"},"outputs":[],"source":["compare_to_model(SELF_10K, SELF_10K)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cnaNxIEBRr-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP+X7q32YzhHt2ab/o4/Si5","collapsed_sections":["rOHKJqHY22Ga"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
