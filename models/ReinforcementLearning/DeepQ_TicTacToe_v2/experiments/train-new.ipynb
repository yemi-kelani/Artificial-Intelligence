{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XReCdM8kILUt"
   },
   "source": [
    "#### Optimal Opponent Experiments\n",
    "Author: Yemi Kelani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vNSWFUkwdXc"
   },
   "source": [
    "##### Google Drive Setup (Skip if running locally)\n",
    "\n",
    "> To run this notebook, follow these steps:\n",
    "> 1. Download the latest version of the [repository](https://github.com/yemi-kelani/artificial-intelligence/tree/master).\n",
    "> 2. Upload the repsitory files to your Google Drive account under the path `Projects/artificial-intelligence`.\n",
    "> 3. Open this file (`train.ipynb`) from your Google Drive and run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17125,
     "status": "ok",
     "timestamp": 1730866117884,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "rEqTNnEdl-8u",
    "outputId": "c4d16bbd-fd7d-46b6-8558-e3c3f72eb41a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730866117884,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "3IlDuGfgof5R"
   },
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"drive/MyDrive/Projects/artificial-intelligence/models/ReinforcementLearning/\"\n",
    "PROJECT_PATH = f\"{ROOT_FOLDER}/DeepQ_TicTacToe_v2\"\n",
    "NOTEBOOK_LOCATION = f\"{PROJECT_PATH}/experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5627,
     "status": "ok",
     "timestamp": 1730866353934,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "5dji1umepw8Z"
   },
   "outputs": [],
   "source": [
    "!cp {PROJECT_PATH}/DeepQAgent.py .\n",
    "!cp {PROJECT_PATH}/TicTacToeGame.py .\n",
    "!cp {ROOT_FOLDER}/Utils.py .\n",
    "\n",
    "from DeepQAgent import DeepQAgent\n",
    "from TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n",
    "from Utils import (\n",
    "    train_agent,\n",
    "    test_agent\n",
    ")\n",
    "MODEL_PATH = \"drive/MyDrive/Projects/artificial-intelligence/trained_models/ReinforcementLearning/TicTacToeV2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local Setup (Skip if running remotely)\n",
    "\n",
    "> 1. Run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S8BLUQ7N3Ors"
   },
   "outputs": [],
   "source": [
    "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent\n",
    "from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n",
    "from models.ReinforcementLearning.Utils import (\n",
    "    train_agent,\n",
    "    test_agent\n",
    ")\n",
    "MODEL_PATH = \"../../../../trained_models/ReinforcementLearning/TicTacToeV2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kw4XZB63VyB"
   },
   "source": [
    "##### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1730866380936,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "IpvCCHUX3Ge2",
    "outputId": "4c3b7b11-0c5b-46ce-d567-ddb81d290876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# DeepQ parameters\n",
    "BATCH_SIZE     = 256\n",
    "NUM_EPISODES   = 10000\n",
    "STATE_SPACE    = 9\n",
    "ACTION_SPACE   = 9\n",
    "HIDDEN_SIZE    = 128\n",
    "EPSILON        = 1.0\n",
    "GAMMA          = 0.95\n",
    "LEARNING_RATE  = 0.001\n",
    "MOEMENTUM      = 0.90\n",
    "DROPOUT        = 0.20\n",
    "TRAIN_START    = 1000 # =< 2000 (Maxlen of replay memory)\n",
    "\n",
    "# model roots\n",
    "BASELINE = \"TicTacToe-v2-BASELINE\"\n",
    "NAIVE = \"TicTacToe-v2-NAIVE\"\n",
    "AGENT = \"TicTacToe-v2-AGENT\"\n",
    "OPTIMAL = \"TicTacToe-v2-OPTIMAL\"\n",
    "SELF = \"TicTacToe-v2-SELF\"\n",
    "\n",
    "def get_full_model_path(agent_name: str = None):\n",
    "  if agent_name is None:\n",
    "    return os.path.join(MODEL_PATH, \"\" + \".pt\")\n",
    "  return os.path.join(MODEL_PATH, agent_name + \".pt\")\n",
    "\n",
    "def supply_model(\n",
    "  load_if_exists: bool = True, \n",
    "  agent_name: str = None,\n",
    "  optimizer_type = \"\"\n",
    "  ):\n",
    "\n",
    "  agent = DeepQAgent(\n",
    "      device         = DEVICE,\n",
    "      epsilon        = EPSILON,\n",
    "      gamma          = GAMMA,\n",
    "      state_space    = STATE_SPACE,\n",
    "      action_space   = ACTION_SPACE,\n",
    "      hidden_size    = HIDDEN_SIZE,\n",
    "      dropout        = DROPOUT,\n",
    "      train_start    = TRAIN_START,\n",
    "      batch_size     = BATCH_SIZE,\n",
    "  )\n",
    "\n",
    "  full_model_path = get_full_model_path(agent_name)\n",
    "  if load_if_exists and os.path.exists(full_model_path):\n",
    "    print(\"Loading Model Parameters...\")\n",
    "    agent.load_model(filepath=full_model_path)\n",
    "\n",
    "  match optimizer_type.upper():\n",
    "    case \"SGD\":\n",
    "      optimizer = torch.optim.SGD(\n",
    "        agent.parameters(), \n",
    "        lr=LEARNING_RATE, \n",
    "        momentum=MOEMENTUM\n",
    "      )\n",
    "    case \"RMS\":\n",
    "      optimizer = torch.optim.RMSprop(agent.parameters(), lr=LEARNING_RATE)\n",
    "    case \"ADAM\":\n",
    "      optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "    case _:\n",
    "      optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "      \n",
    "  # criterion = torch.nn.SmoothL1Loss() # Huber Loss\n",
    "  # criterion = torch.nn.MSELoss()\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  \n",
    "  return agent, optimizer, criterion\n",
    "\n",
    "def compare_to_naive(agent_name: str, num_episodes: int = 10000):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n",
    "\n",
    "def compare_to_optimal(agent_name: str, num_episodes: int = 100):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n",
    "\n",
    "def compare_to_model(agent_name: str, model_name: str, num_episodes: int = 10000):\n",
    "  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n",
    "  _enemy, _, _ = supply_model(load_if_exists=True, agent_name=model_name)\n",
    "  _environment = TicTacToeGame(DEVICE, _enemy, OPPONENT_LEVEL.AGENT, start_as_X=False)\n",
    "  test_agent(_agent, _environment, num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 5852,
     "status": "ok",
     "timestamp": 1730780311679,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "zgX_xdLvxaaE",
    "outputId": "d27812ce-7610-46d3-d9fe-1badef8b5b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to '../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent, _, _ = supply_model()\n",
    "agent.save_model(MODEL_PATH, BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68624,
     "status": "ok",
     "timestamp": 1730780380300,
     "user": {
      "displayName": "A. Kelani",
      "userId": "12102654022096104534"
     },
     "user_tz": 480
    },
    "id": "KgNZkepzxkQ9",
    "outputId": "cdbd3b32-5b3f-4834-c9ec-6770290ea1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model Parameters...\n",
      "Model loaded from '../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 702.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  44.6%\n",
      "Draw rate: 7.42%\n",
      "Loss rate: 47.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_to_naive(BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8fX711p3rw_",
    "outputId": "1eff47fb-57b7-4c77-d996-aecbf5ee210f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model Parameters...\n",
      "Model loaded from '../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'.\n",
      "Copied weights from policy network to target network.\n",
      "episode: 1/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 2/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 3/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|X|O| |\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 4/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "| |X| |\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 5/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |X|\n",
      "| |X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 6/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|X|O|X|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 7/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 8/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |O|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 9/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 10/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 11/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 12/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 13/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 14/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O| |\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 15/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 16/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 17/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 18/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 19/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 20/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|O|X|X|\n",
      "| |O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 21/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 22/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|O|\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 23/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 24/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "| |O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 25/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|X|\n",
      "|O|X|O|\n",
      "| |X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 26/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 27/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 28/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 29/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |X|\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 30/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 31/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "| | |O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 32/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 33/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|O|X|X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 34/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 35/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O| |\n",
      "|X|O| |\n",
      "| |O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 36/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|O|\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 37/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|X| |O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 38/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O| |\n",
      "|O|X| |\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 39/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "| |O|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 40/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 41/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 42/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|O| |X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 43/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 44/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "| |O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 45/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 46/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X| |\n",
      "|X| |O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 47/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|O|\n",
      "| |X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 48/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 49/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "| |X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 50/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 51/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 52/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "| |O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 53/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 54/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 55/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O| |\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 56/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|X|X| |\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 57/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |O|X|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 58/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 59/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|X|O|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 60/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| | |\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 61/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 62/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 63/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | | |\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 64/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 65/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 66/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 67/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 68/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O| |\n",
      "| |O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 69/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|X| |O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 70/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 71/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 72/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 73/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O| |O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 74/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 75/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |O|\n",
      "| |X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 76/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 77/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 78/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|X|\n",
      "|O|X| |\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 79/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "|O|X| |\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 80/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 81/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 82/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O| |\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 83/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "| |X|O|\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 84/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 85/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| | |\n",
      "|X|O|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 86/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 87/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "| |O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 88/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 89/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "| | |X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 90/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 91/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|O|\n",
      "| |O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 92/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 93/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 94/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "| |X|O|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 95/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "| |X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 96/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|X|O| |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 97/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 98/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X| |\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 99/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 100/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|X|\n",
      "|O|O|X|\n",
      "| |O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 101/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 102/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "| |X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 103/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O| |X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 104/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 105/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "| |X|O|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 106/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O| |\n",
      "|O|X| |\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 107/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O| |\n",
      "| |X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 108/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 109/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 110/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 111/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O| | |\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 112/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X| |\n",
      "|X|X|O|\n",
      "| |X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 113/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "| |O|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 114/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 115/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 116/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 117/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |X|\n",
      "|O|X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 118/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 119/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 120/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "|O|X| |\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 121/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X| |\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 122/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|O|X|O|\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 123/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O| |\n",
      "| |O| |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 124/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 125/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 126/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X| |\n",
      "|O|X| |\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 127/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 128/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|O|X|X|\n",
      "| |O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 129/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 130/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 131/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 132/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 133/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "| |X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 134/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 135/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|X|\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 136/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|O|X|X|\n",
      "| |O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 137/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 138/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 139/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "| |X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 140/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 141/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|X| |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 142/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X| |\n",
      "|O|X| |\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 143/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |X|\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 144/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 145/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 146/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|O|\n",
      "|O|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 147/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 148/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X| |\n",
      "| |X|X|\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 149/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|X|O|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 150/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |O|\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 151/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 152/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |O|\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 153/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 154/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 155/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X| |\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 156/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "| |O| |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 157/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 158/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 159/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 160/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |X| |\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 161/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 162/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 163/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 164/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 165/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|O|\n",
      "|X|X|X|\n",
      "| |X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 166/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 167/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|O|O|X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 168/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 169/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 170/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 171/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "| |O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 172/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 173/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 174/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 175/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 176/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 177/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 178/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |X|\n",
      "|X|O|X|\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 179/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | | |\n",
      "|O| |O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 180/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 181/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|O|\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 182/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "| |X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 183/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |O| |\n",
      "| |O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 184/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 185/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 186/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 187/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 188/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 189/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 190/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X| |X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 191/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 192/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 193/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 194/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 195/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|X|\n",
      "|X|X|O|\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 196/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 197/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 198/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O| |\n",
      "|X| |O|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 199/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "| |X| |\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 200/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 201/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 202/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 203/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "| |O|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 204/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|X|O| |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 205/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O| |\n",
      "|X|O| |\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 206/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 207/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 208/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 209/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 210/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 211/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 212/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 213/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | |X|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 214/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 215/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 216/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "| |O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 217/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 218/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|O|\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 219/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O| |\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 220/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 221/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 222/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 223/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 224/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |O|\n",
      "|X|X|X|\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 225/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 226/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 227/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |X|\n",
      "|O|X|X|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 228/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 229/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 230/10000, steps: 5, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 231/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X| |\n",
      "|O|X|X|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 232/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 233/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| | |\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 234/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "| |X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 235/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 236/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O| |\n",
      "|X|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 237/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O| |\n",
      "|X|O| |\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 238/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X| |\n",
      "|O| |O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 239/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|X|O|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 240/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 241/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "| |X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 242/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|X|\n",
      "|O|X|O|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 243/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 244/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |O|O|\n",
      "| |X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 245/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 246/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| |X|\n",
      "| |X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 247/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 248/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| | |\n",
      "| |X| |\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 249/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 250/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|X|\n",
      "| | |X|\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 251/10000, steps: 3, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O| | |\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 252/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O| |\n",
      "|X| |O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 253/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 254/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| |X|O|\n",
      "|O|X|X|\n",
      "| |X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 255/10000, steps: 4, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 256/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 257/10000, steps: 4, reward_total: 0, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 258/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 259/10000, steps: 2, reward_total: -1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "| | | |\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 260/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 261/10000, steps: 4, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 262/10000, steps: 5, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|X|O|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 263/10000, steps: 3, reward_total: 1, loss_avg: n/a, e: 1.0, time: 12:22:42\n",
      "_______\n",
      "|X| |X|\n",
      "|X| | |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 264/10000, steps: 4, reward_total: 1, loss_avg: -56.42430114746094, e: 0.001, time: 12:22:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X| |\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 265/10000, steps: 3, reward_total: -1, loss_avg: -6498.3876953125, e: 0.001, time: 12:22:43\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|X| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 266/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:43\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 267/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:43\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 268/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:43\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 269/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:43\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 270/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:44\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 271/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:44\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 272/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:44\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 273/10000, steps: 4, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:22:44\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 274/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:45\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 275/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:45\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 276/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:45\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 277/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:45\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 278/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:45\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 279/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:46\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 280/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:46\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 281/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:46\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 282/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:46\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 283/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:46\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 284/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:47\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 285/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:47\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 286/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:47\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 287/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:47\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 288/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:47\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 289/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:48\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 290/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:48\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 291/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:48\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 292/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:48\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 293/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:48\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 294/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:49\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 295/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:49\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 296/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:49\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 297/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:49\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 298/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:49\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 299/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:50\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 300/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:50\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 301/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:50\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 302/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:50\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 303/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:51\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 304/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:51\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 305/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:51\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 306/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:51\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 307/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:51\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 308/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:52\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 309/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:52\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 310/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:52\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 311/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:52\n",
      "_______\n",
      "|O|O|X|\n",
      "| |X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 312/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:52\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 313/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:53\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 314/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:53\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 315/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:53\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 316/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:53\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 317/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:53\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 318/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:54\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 319/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:54\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 320/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:54\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 321/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:54\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 322/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:54\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 323/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:55\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 324/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:55\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 325/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:55\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 326/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:55\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 327/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:55\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 328/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:56\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 329/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:56\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 330/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:56\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 331/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:56\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 332/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:56\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 333/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:57\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 334/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:57\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 335/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:57\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| |X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 336/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:57\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 337/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:58\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 338/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:58\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 339/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:58\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 340/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:58\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 341/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:58\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 342/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:59\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 343/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:59\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 344/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:22:59\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 345/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:22:59\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 346/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:22:59\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 347/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:00\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 348/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:00\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 349/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:00\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 350/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:00\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 351/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:01\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 352/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:01\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 353/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:01\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 354/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:01\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 355/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:01\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 356/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:02\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 357/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:02\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 358/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:02\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 359/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:02\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 360/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:02\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 361/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:03\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 362/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:03\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 363/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:03\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 364/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:03\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 365/10000, steps: 4, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:03\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 366/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:04\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 367/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:04\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 368/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:04\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 369/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:04\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 370/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:05\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 371/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:05\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 372/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:05\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 373/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:05\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 374/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:05\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 375/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:05\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 376/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:06\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 377/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:06\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 378/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:06\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 379/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:06\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 380/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:07\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 381/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:07\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 382/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:07\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 383/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:07\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 384/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:08\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 385/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:08\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 386/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:08\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 387/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:08\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 388/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:08\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 389/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:09\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 390/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:09\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 391/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:09\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 392/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:09\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 393/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:10\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 394/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:10\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 395/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:10\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 396/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:10\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 397/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:11\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 398/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:11\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 399/10000, steps: 4, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:11\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 400/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:11\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 401/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:11\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 402/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:12\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 403/10000, steps: 4, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:12\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 404/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:12\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 405/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:12\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 406/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:13\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 407/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:13\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 408/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:13\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 409/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:13\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 410/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:13\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 411/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:14\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 412/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:14\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 413/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:14\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 414/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:14\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 415/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:14\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 416/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:15\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 417/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:15\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 418/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:15\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 419/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:15\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 420/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:15\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 421/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:16\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 422/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:16\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 423/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:16\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 424/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:16\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 425/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:16\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 426/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:17\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 427/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:17\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 428/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:17\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 429/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:17\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 430/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:17\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 431/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:18\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 432/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:18\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 433/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:18\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 434/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:18\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 435/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:18\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 436/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:19\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 437/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:19\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 438/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:19\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 439/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:19\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 440/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:20\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 441/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:20\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 442/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:20\n",
      "_______\n",
      "|X|X|O|\n",
      "|X| |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 443/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:20\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 444/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:20\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 445/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:21\n",
      "_______\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 446/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:21\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 447/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:21\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| |X|\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 448/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:21\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 449/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:21\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 450/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:22\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 451/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:22\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 452/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:22\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 453/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:22\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 454/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:22\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 455/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:23\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 456/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:23\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 457/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:23\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 458/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:23\n",
      "_______\n",
      "|X|X|O|\n",
      "|X| |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 459/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:24\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 460/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:24\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 461/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:24\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 462/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:24\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 463/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:24\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 464/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:25\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 465/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:25\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 466/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:25\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 467/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:25\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 468/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:25\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 469/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:26\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 470/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:26\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 471/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:26\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 472/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:26\n",
      "_______\n",
      "|X|X|O|\n",
      "|X| |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 473/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:26\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 474/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:27\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 475/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:27\n",
      "_______\n",
      "|X|O|O|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 476/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:27\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 477/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:27\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 478/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:27\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 479/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:28\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 480/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:28\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 481/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:28\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 482/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:28\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 483/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:28\n",
      "_______\n",
      "|O|O|X|\n",
      "| |X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 484/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:29\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 485/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:29\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 486/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:29\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 487/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:29\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 488/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:29\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 489/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:30\n",
      "_______\n",
      "|X|O|O|\n",
      "| |X| |\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 490/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:30\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 491/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:30\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 492/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:30\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 493/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:30\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 494/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:31\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 495/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:31\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 496/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:31\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 497/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:31\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 498/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:31\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 499/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:32\n",
      "_______\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 500/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:32\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 501/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:32\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 502/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:32\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 503/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:32\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 504/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:33\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 505/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:33\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 506/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:33\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 507/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:33\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 508/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:33\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 509/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:34\n",
      "_______\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 510/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:34\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 511/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:34\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 512/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:34\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 513/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:34\n",
      "_______\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 514/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:35\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 515/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:35\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 516/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:35\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 517/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:35\n",
      "_______\n",
      "|X|O|O|\n",
      "|X|O|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 518/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:35\n",
      "_______\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 519/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:35\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 520/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:36\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 521/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:36\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 522/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:36\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 523/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:36\n",
      "_______\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 524/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:36\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 525/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:37\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 526/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:37\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 527/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:37\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 528/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:37\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 529/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:37\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 530/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:38\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 531/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:38\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 532/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:38\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 533/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:38\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 534/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:38\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 535/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:39\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 536/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:39\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 537/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:39\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 538/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:39\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 539/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:39\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 540/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:40\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 541/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:40\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 542/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:40\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 543/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:40\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 544/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:40\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 545/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:41\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 546/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:41\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 547/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:41\n",
      "_______\n",
      "|X|O|O|\n",
      "|O|X|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 548/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:41\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 549/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 550/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:42\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 551/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:42\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 552/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:42\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 553/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:42\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| |X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 554/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:43\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 555/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:43\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 556/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:43\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 557/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:43\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 558/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:43\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 559/10000, steps: 4, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:44\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 560/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:44\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 561/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:44\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 562/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:44\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 563/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:44\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 564/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:45\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 565/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:45\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 566/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:45\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 567/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:45\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 568/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:45\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 569/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:46\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 570/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:46\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 571/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:46\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 572/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:46\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 573/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:46\n",
      "_______\n",
      "|O|X|O|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 574/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:47\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 575/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:47\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 576/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:47\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 577/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:47\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 578/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:47\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 579/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:48\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 580/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:48\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 581/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:48\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 582/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:48\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 583/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:48\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 584/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:49\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 585/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:49\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 586/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:49\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "| |O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 587/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:49\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 588/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:49\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 589/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:50\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 590/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:50\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 591/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:50\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 592/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:50\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 593/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:50\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 594/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:51\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 595/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:51\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 596/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:51\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 597/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:51\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 598/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:51\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 599/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:52\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 600/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:52\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 601/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:52\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 602/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:52\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 603/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:52\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 604/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:53\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 605/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:53\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 606/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:53\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 607/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:53\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 608/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:54\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X|O|\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 609/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:54\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 610/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:54\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 611/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:54\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 612/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:54\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 613/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:55\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 614/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:55\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 615/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:55\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 616/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:55\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 617/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:55\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 618/10000, steps: 5, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:56\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 619/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:56\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 620/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:56\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 621/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:56\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 622/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:56\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 623/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:57\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 624/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:57\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 625/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:57\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 626/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:57\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 627/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:57\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 628/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:58\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|O|X|\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 629/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:58\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 630/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:23:58\n",
      "_______\n",
      "|X|X|O|\n",
      "|O|X|X|\n",
      "|X|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 631/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:58\n",
      "_______\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 632/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:59\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 633/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:23:59\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 634/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:59\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 635/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:59\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 636/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:23:59\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 637/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:00\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 638/10000, steps: 5, reward_total: 0, loss_avg: nan, e: 0.001, time: 12:24:00\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X|O|\n",
      "|O|X|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 639/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:00\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 640/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:00\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 641/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:01\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X| |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 642/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:01\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 643/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:01\n",
      "_______\n",
      "|O|O|X|\n",
      "| |X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 644/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:01\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 645/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:01\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 646/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:01\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 647/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:02\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 648/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:02\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 649/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:02\n",
      "_______\n",
      "|O|O|O|\n",
      "| |X|X|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 650/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:02\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 651/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:03\n",
      "_______\n",
      "|O|X|X|\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 652/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:03\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 653/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:03\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| | |\n",
      "|X|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 654/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:03\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 655/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:03\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 656/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:04\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 657/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:04\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "| |X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 658/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:04\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 659/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:04\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 660/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:04\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 661/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:05\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 662/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:05\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 663/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:05\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 664/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:05\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 665/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:05\n",
      "_______\n",
      "|O|X|O|\n",
      "|O| | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 666/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:06\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 667/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:06\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 668/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:06\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 669/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:06\n",
      "_______\n",
      "|O|O|O|\n",
      "|X|X| |\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 670/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:06\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 671/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:06\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 672/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:07\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| | |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 673/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:07\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 674/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:07\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 675/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:07\n",
      "_______\n",
      "|O|O| |\n",
      "| | | |\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 676/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:07\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 677/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:08\n",
      "_______\n",
      "|O|O|X|\n",
      "| | |X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 678/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:08\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 679/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:08\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|X|\n",
      "|O| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 680/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:08\n",
      "_______\n",
      "|X|X|O|\n",
      "|X|O|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 681/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:08\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|X|\n",
      "|X|O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 682/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:09\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 683/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:09\n",
      "_______\n",
      "|O|O|O|\n",
      "| | |X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 684/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:09\n",
      "_______\n",
      "|X|O|X|\n",
      "|X|X| |\n",
      "|O|O|O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 685/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:09\n",
      "_______\n",
      "|O|O|O|\n",
      "|X| |X|\n",
      "| |X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 686/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:09\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 687/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:10\n",
      "_______\n",
      "|O|O| |\n",
      "|X|X|X|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 688/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:10\n",
      "_______\n",
      "|X|X|O|\n",
      "|X| |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 689/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:10\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|O|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 690/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:10\n",
      "_______\n",
      "|X|X|X|\n",
      "|O|O| |\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 691/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:10\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|X|X|\n",
      "| | |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 692/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:11\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 693/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:11\n",
      "_______\n",
      "|O|O|X|\n",
      "|X|X|O|\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 694/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:11\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O|O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 695/10000, steps: 4, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:11\n",
      "_______\n",
      "|O|X|O|\n",
      "|X|O|X|\n",
      "|O|X| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 696/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:11\n",
      "_______\n",
      "|X|X|X|\n",
      "|O| |O|\n",
      "| | | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 697/10000, steps: 2, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:12\n",
      "_______\n",
      "|O|O|X|\n",
      "| |X| |\n",
      "|X| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 698/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:12\n",
      "_______\n",
      "|X|X|O|\n",
      "|X| |O|\n",
      "| | |O|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 699/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:12\n",
      "_______\n",
      "|O|X|O|\n",
      "|O|X|O|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 700/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:12\n",
      "_______\n",
      "|X|X|X|\n",
      "| | |O|\n",
      "| |O| |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 701/10000, steps: 4, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:13\n",
      "_______\n",
      "|O|O|X|\n",
      "|O|O|X|\n",
      "|X|X|X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 702/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:13\n",
      "_______\n",
      "|X|X|X|\n",
      "| |O| |\n",
      "|O| | |\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 703/10000, steps: 3, reward_total: -1, loss_avg: nan, e: 0.001, time: 12:24:13\n",
      "_______\n",
      "|O|O|X|\n",
      "|O| |X|\n",
      "|X| |X|\n",
      "‾‾‾‾‾‾‾\n",
      "episode: 704/10000, steps: 3, reward_total: 1, loss_avg: nan, e: 0.001, time: 12:24:13\n",
      "_______\n",
      "|X|X|X|\n",
      "| | | |\n",
      "|O| |O|\n",
      "‾‾‾‾‾‾‾\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# agent.prep_cosine_anneal(0.0, 1.0, NUM_EPISODES)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m environment \u001b[38;5;241m=\u001b[39m TicTacToeGame(DEVICE, \u001b[38;5;28;01mNone\u001b[39;00m, OPPONENT_LEVEL\u001b[38;5;241m.\u001b[39mNAIVE)\n\u001b[0;32m----> 8\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_EPISODES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNAIVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# epsilon_min_value = 0.30,\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# epsilon_max_value = 0.75,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_loss_history()\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence/models/ReinforcementLearning/Utils.py:84\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, environment, num_episodes, optimizer, criterion, device, save_path, model_name, save_every, epsilon_min_value, epsilon_max_value)\u001b[0m\n\u001b[1;32m     82\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m     83\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     87\u001b[0m reward_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence/models/ReinforcementLearning/DeepQ_TicTacToe_v2/DeepQAgent.py:200\u001b[0m, in \u001b[0;36mDeepQAgent.replay\u001b[0;34m(self, optimizer, criterion, episode)\u001b[0m\n\u001b[1;32m    198\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(torch\u001b[38;5;241m.\u001b[39mstack(q_batch), torch\u001b[38;5;241m.\u001b[39mstack(target_q_batch))\n\u001b[1;32m    199\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 200\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# gradient clipping in-place\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_value_(self.policy_network.parameters(), 1.0)\u001b[39;00m\n\u001b[1;32m    205\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/artificial-intelligence/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent, optimizer, criterion = supply_model(\n",
    "    load_if_exists=True, \n",
    "    agent_name=BASELINE,\n",
    "    optimizer_type=\"SGD\"\n",
    ")\n",
    "agent.prep_cosine_anneal(0.0, 1.0, NUM_EPISODES)\n",
    "environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n",
    "reward_history = train_agent(\n",
    "    agent,\n",
    "    environment,\n",
    "    NUM_EPISODES,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    MODEL_PATH,\n",
    "    model_name = NAIVE,\n",
    "    save_every = 2000,\n",
    "    # epsilon_min_value = 0.30,\n",
    "    # epsilon_max_value = 0.75,\n",
    ")\n",
    "agent.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 894.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  57.11%\n",
      "Draw rate: 9.71%\n",
      "Loss rate: 33.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:01<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  0.0%\n",
      "Draw rate: 0.0%\n",
      "Loss rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-2K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-2K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 892.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  50.32%\n",
      "Draw rate: 12.92%\n",
      "Loss rate: 36.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  0.0%\n",
      "Draw rate: 0.0%\n",
      "Loss rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-4K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-4K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 882.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  47.17%\n",
      "Draw rate: 15.62%\n",
      "Loss rate: 37.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:02<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  0.0%\n",
      "Draw rate: 4.0%\n",
      "Loss rate: 96.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-6K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-6K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 889.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  54.6%\n",
      "Draw rate: 8.04%\n",
      "Loss rate: 37.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:05<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  0.0%\n",
      "Draw rate: 0.0%\n",
      "Loss rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-8K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-8K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 912.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  68.63%\n",
      "Draw rate: 5.3%\n",
      "Loss rate: 26.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:02<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Win rate:  0.0%\n",
      "Draw rate: 14.0%\n",
      "Loss rate: 86.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_to_naive(f\"{OPTIMAL}-10K\")\n",
    "compare_to_optimal(f\"{OPTIMAL}-10K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(torch.tensor([[[0,1,2],[3,4,5],[6,7,8]], [[0,1,2],[3,4,5],[6,7,8]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack([\n",
    "    torch.tensor([1,2,3,4,5,6,7,8,9]).reshape((1, 9)),\n",
    "    torch.tensor([1,2,3,4,5,6,7,8,9]).reshape((1, 9)),\n",
    "    torch.tensor([1,2,3,4,5,6,7,8,9]).reshape((1, 9)),\n",
    "    torch.tensor([1,2,3,4,5,6,7,8,9]).reshape((1, 9))\n",
    "])\n",
    "\n",
    "batch.squeeze().shape\n",
    "\n",
    "# batch.reshape((len(batch), len(batch[0]))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3, 3)).reshape((1, agent.action_space)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.zeros((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4,5,6,7,8,9]).reshape((1, 9)).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,5][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnbY0pxlm6JTS4enhO6PPu",
   "collapsed_sections": [
    "rOHKJqHY22Ga"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
