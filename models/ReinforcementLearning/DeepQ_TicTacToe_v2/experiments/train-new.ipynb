{"cells":[{"cell_type":"markdown","metadata":{"id":"XReCdM8kILUt"},"source":["#### Optimal Opponent Experiments\n","Author: Yemi Kelani"]},{"cell_type":"markdown","metadata":{"id":"1vNSWFUkwdXc"},"source":["##### Google Drive Setup (Skip if running locally)\n","\n","> To run this notebook, follow these steps:\n","> 1. Download the latest version of the [repository](https://github.com/yemi-kelani/artificial-intelligence/tree/master).\n","> 2. Upload the repsitory files to your Google Drive account under the path `Projects/artificial-intelligence`.\n","> 3. Open this file (`train.ipynb`) from your Google Drive and run the experiments."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17125,"status":"ok","timestamp":1730866117884,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"rEqTNnEdl-8u","outputId":"c4d16bbd-fd7d-46b6-8558-e3c3f72eb41a"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1730866117884,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"3IlDuGfgof5R"},"outputs":[],"source":["ROOT_FOLDER = \"drive/MyDrive/Projects/artificial-intelligence/models/ReinforcementLearning/\"\n","PROJECT_PATH = f\"{ROOT_FOLDER}/DeepQ_TicTacToe_v2\"\n","NOTEBOOK_LOCATION = f\"{PROJECT_PATH}/experiments\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5627,"status":"ok","timestamp":1730866353934,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"5dji1umepw8Z"},"outputs":[],"source":["!cp {PROJECT_PATH}/DeepQAgent.py .\n","!cp {PROJECT_PATH}/TicTacToeGame.py .\n","!cp {ROOT_FOLDER}/Utils.py .\n","\n","from DeepQAgent import DeepQAgent\n","from TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n","from Utils import (\n","    train_agent,\n","    test_agent\n",")\n","MODEL_PATH = \"drive/MyDrive/Projects/artificial-intelligence/trained_models/ReinforcementLearning/TicTacToeV2\""]},{"cell_type":"markdown","metadata":{"id":"rOHKJqHY22Ga"},"source":["##### Local Setup (Skip if running remotely)\n","> 1. Run the following cells"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"S8BLUQ7N3Ors"},"outputs":[],"source":["from models.ReinforcementLearning.DeepQ_TicTacToe_v2.DeepQAgent import DeepQAgent\n","from models.ReinforcementLearning.DeepQ_TicTacToe_v2.TicTacToeGame import TicTacToeGame, OPPONENT_LEVEL\n","from models.ReinforcementLearning.Utils import (\n","    train_agent,\n","    test_agent\n",")\n","MODEL_PATH = \"../../../../trained_models/ReinforcementLearning/TicTacToeV2\""]},{"cell_type":"markdown","metadata":{"id":"2kw4XZB63VyB"},"source":["##### Experiments"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1730866380936,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"IpvCCHUX3Ge2","outputId":"4c3b7b11-0c5b-46ce-d567-ddb81d290876"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["import os\n","import torch\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)\n","\n","# DeepQ parameters\n","BATCH_SIZE     = 256\n","NUM_EPISODES   = 10000\n","STATE_SPACE    = 9\n","ACTION_SPACE   = 9\n","HIDDEN_SIZE    = 256\n","EPSILON        = 1.0\n","GAMMA          = 0.90\n","LEARNING_RATE  = 0.001\n","DROPOUT        = 0.15\n","TRAIN_START    = 1000 # =< 2000 (Maxlen of replay memory)\n","NEGATIVE_SLOPE = 0.01\n","\n","# model roots\n","BASELINE = \"TicTacToe-v2-BASELINE\"\n","NAIVE = \"TicTacToe-v2-NAIVE\"\n","AGENT = \"TicTacToe-v2-AGENT\"\n","OPTIMAL = \"TicTacToe-v2-OPTIMAL\"\n","SELF = \"TicTacToe-v2-SELF\"\n","\n","\n","def get_full_model_path(agent_name: str = None):\n","  if agent_name is None:\n","    return os.path.join(MODEL_PATH, \"\" + \".pt\")\n","  return os.path.join(MODEL_PATH, agent_name + \".pt\")\n","\n","def supply_model(load_if_exists: bool = True, agent_name: str = None):\n","\n","  agent = DeepQAgent(\n","      device         = DEVICE,\n","      epsilon        = EPSILON,\n","      gamma          = GAMMA,\n","      state_space    = STATE_SPACE,\n","      action_space   = ACTION_SPACE,\n","      hidden_size    = HIDDEN_SIZE,\n","      dropout        = DROPOUT,\n","      train_start    = TRAIN_START,\n","      batch_size     = BATCH_SIZE,\n","      negative_slope = NEGATIVE_SLOPE\n","  )\n","\n","  full_model_path = get_full_model_path(agent_name)\n","  if load_if_exists and os.path.exists(full_model_path):\n","    print(\"Loading Model Parameters...\")\n","    agent.load_model(filepath=full_model_path)\n","\n","  optimizer = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n","  criterion = torch.nn.SmoothL1Loss() # Huber Loss\n","\n","  return agent, optimizer, criterion\n","\n","def compare_to_naive(agent_name: str, num_episodes: int = 10000):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n","\n","def compare_to_optimal(agent_name: str, num_episodes: int = 100):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.OPTIMAL, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n","\n","def compare_to_model(agent_name: str, model_name: str, num_episodes: int = 10000):\n","  _agent, _, _ = supply_model(load_if_exists=True, agent_name=agent_name)\n","  _enemy, _, _ = supply_model(load_if_exists=True, agent_name=model_name)\n","  _environment = TicTacToeGame(DEVICE, _enemy, OPPONENT_LEVEL.AGENT, start_as_X=False)\n","  test_agent(_agent, _environment, num_episodes)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":5852,"status":"ok","timestamp":1730780311679,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"zgX_xdLvxaaE","outputId":"d27812ce-7610-46d3-d9fe-1badef8b5b5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to '../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'.\n"]},{"data":{"text/plain":["'../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["agent, _, _ = supply_model()\n","agent.save_model(MODEL_PATH, BASELINE)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68624,"status":"ok","timestamp":1730780380300,"user":{"displayName":"A. Kelani","userId":"12102654022096104534"},"user_tz":480},"id":"KgNZkepzxkQ9","outputId":"cdbd3b32-5b3f-4834-c9ec-6770290ea1ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/yemikelani/Projects/Artificial Intelligence/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"name":"stdout","output_type":"stream","text":["Loading Model Parameters...\n","Model loaded from '../../../../trained_models/ReinforcementLearning/TicTacToeV2/TicTacToe-v2-BASELINE.pt'.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [00:12<00:00, 810.19it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Win rate:  58.5%\n","Draw rate: 9.49%\n","Loss rate: 32.01%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["compare_to_naive(BASELINE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8fX711p3rw_","outputId":"1eff47fb-57b7-4c77-d996-aecbf5ee210f"},"outputs":[],"source":["agent, optimizer, criterion = supply_model(load_if_exists=True, agent_name=BASELINE)\n","agent.prep_cosine_anneal(0.2, 1.0, NUM_EPISODES)\n","environment = TicTacToeGame(DEVICE, None, OPPONENT_LEVEL.NAIVE)\n","reward_history = train_agent(\n","    agent,\n","    environment,\n","    NUM_EPISODES,\n","    optimizer,\n","    criterion,\n","    DEVICE,\n","    MODEL_PATH,\n","    model_name = NAIVE,\n","    save_every = 2000,\n","    # epsilon_min_value = 0.30,\n","    # epsilon_max_value = 0.75,\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOnbY0pxlm6JTS4enhO6PPu","collapsed_sections":["rOHKJqHY22Ga"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
